{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "openai_api_key = getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nHello! Thank you for letting me know. Is there anything else you would like to test or discuss?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('hello, this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "\n",
       "Hello! Thank you for letting me know. Is there anything else you would like to test or discuss?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(llm(\"hello, this is a test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Zero-shot chain-of-thought prompting is a technique used in natural language processing (NLP) to generate coherent and relevant responses to a given prompt without any prior training or specific instructions. It involves using a pre-trained language model to generate a sequence of words or phrases that are related to the prompt, without being explicitly trained on the specific task or topic. This allows the model to generate responses that are not limited by a specific set of prompts or topics, making it more versatile and adaptable to different scenarios."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm(\"What is zero-shot chain-of-thought prompting?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pretrained large language models (LLMs) are widely used in many sub-fields of\n",
       "natural language processing (NLP) and generally known as excellent few-shot\n",
       "learners with task-specific exemplars. Notably, chain of thought (CoT)\n",
       "prompting, a recent technique for eliciting complex multi-step reasoning\n",
       "through step-by-step answer examples, achieved the state-of-the-art\n",
       "performances in arithmetics and symbolic reasoning, difficult system-2 tasks\n",
       "that do not follow the standard scaling laws for LLMs. While these successes\n",
       "are often attributed to LLMs' ability for few-shot learning, we show that LLMs\n",
       "are decent zero-shot reasoners by simply adding \"Let's think step by step\"\n",
       "before each answer. Experimental results demonstrate that our Zero-shot-CoT,\n",
       "using the same single prompt template, significantly outperforms zero-shot LLM\n",
       "performances on diverse benchmark reasoning tasks including arithmetics\n",
       "(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\n",
       "Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\n",
       "Objects), without any hand-crafted few-shot examples, e.g. increasing the\n",
       "accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\n",
       "large InstructGPT model (text-davinci-002), as well as similar magnitudes of\n",
       "improvements with another off-the-shelf large model, 540B parameter PaLM. The\n",
       "versatility of this single prompt across very diverse reasoning tasks hints at\n",
       "untapped and understudied fundamental zero-shot capabilities of LLMs,\n",
       "suggesting high-level, multi-task broad cognitive capabilities may be extracted\n",
       "by simple prompting. We hope our work not only serves as the minimal strongest\n",
       "zero-shot baseline for the challenging reasoning benchmarks, but also\n",
       "highlights the importance of carefully exploring and analyzing the enormous\n",
       "zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\n",
       "few-shot exemplars."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_client = arxiv.Client()\n",
    "paper = next(arxiv_client.results(arxiv.Search(id_list=[\"2205.11916\"])))\n",
    "\n",
    "Markdown(paper.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Zero-shot chain-of-thought prompting is a technique that utilizes large language models (LLMs) to perform complex multi-step reasoning tasks without any hand-crafted few-shot examples. It involves adding the phrase \"Let's think step by step\" before each answer in a prompt, which significantly improves the zero-shot performance of LLMs on diverse reasoning tasks such as arithmetics, symbolic reasoning, and logical reasoning. This technique suggests that LLMs have untapped and understudied zero-shot capabilities, and highlights the importance of exploring and analyzing these capabilities before creating finetuning datasets or few-shot exemplars."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"Here's a summary of a paper:\n",
    "{paper.summary}\n",
    "\n",
    "Based on that summary, what is zero-shot chain-of-thought prompting?\"\"\"\n",
    "\n",
    "response = llm(prompt)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_path = paper.download_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(paper_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\\n\\n\".join([page.page_content for page in pages[0:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Zero-shot chain-of-thought prompting is a technique for eliciting complex multi-step reasoning through step-by-step answer examples, without the need for task-specific few-shot examples. It involves adding the prompt \"Let's think step by step\" before each answer in order to facilitate step-by-step thinking and improve the performance of large language models on challenging reasoning tasks. This approach is shown to be effective in a variety of tasks, including arithmetic, symbolic, commonsense, and other logical reasoning tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(f\"\"\"Here's the first two pages of a paper:\n",
    "{content}\n",
    "\n",
    "Based on that content, what is zero-shot chain-of-thought prompting?\"\"\")\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"What is zero-shot chain-of-thought prompting?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have par-\n",
       "ticularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\n",
       "(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\n",
       "by modifying the answers in few-shot examples to step-by-step answers, and achieved signiﬁcant\n",
       "boosts in performance across these difﬁcult benchmarks, especially when combined with very large\n",
       "language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\n",
       "few-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\n",
       "given for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\n",
       "in the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\n",
       "asFew-shot-CoT in this work.\n",
       "3 Zero-shot Chain of Thought"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm, chain_type='stuff')\n",
    "query = \"What is zero-shot chain-of-thought prompting?\"\n",
    "\n",
    "sources = db.similarity_search(query)\n",
    "results = chain({'input_documents':sources, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Zero-shot chain-of-thought prompting is a method for eliciting multi-step reasoning from large language models without requiring step-by-step few-shot examples. It differs from prior prompting methods and has potential biases due to the training data used for large language models. \n",
       "SOURCES: ./2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(results['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
